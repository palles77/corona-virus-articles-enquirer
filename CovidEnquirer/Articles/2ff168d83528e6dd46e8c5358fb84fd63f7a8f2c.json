{
    "paper_id": "2ff168d83528e6dd46e8c5358fb84fd63f7a8f2c",
    "metadata": {
        "title": "",
        "authors": []
    },
    "abstract": [],
    "body_text": [
        {
            "text": "Florian Mock et al. Sun, 2018) , probabilistic models (Galiez et al., 2017) and similarity rankings (Edwards et al., 2016; Ahlgren et al., 2017) . All of these approaches require features with which the input sequence can be classified. The features used for classification are mainly k-mer based on various k sizes between 1-8. In the case of probabilistic models and similarity rankings, not only the viral genomes but also the host genomes have to be analyzed.",
            "cite_spans": [
                {
                    "start": 20,
                    "end": 30,
                    "text": "Sun, 2018)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 54,
                    "end": 75,
                    "text": "(Galiez et al., 2017)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 100,
                    "end": 122,
                    "text": "(Edwards et al., 2016;",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 123,
                    "end": 144,
                    "text": "Ahlgren et al., 2017)",
                    "ref_id": "BIBREF1"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Still today, it is mostly unknown how viruses adapt to new hosts and which mechanisms are responsible for enabling zoonosis (Taubenberger and Kash, 2010; Villordo et al., 2015; Longdon et al., 2014) . Because of this incomplete knowledge, it is likely to choose inappropriate features, i.e., features of little or no biological relevance, which is problematic for the accuracy of machine learning approaches. In contrast to classic machine learning approaches, deep neural networks can learn features necessary for solving a specific task by themselves.",
            "cite_spans": [
                {
                    "start": 124,
                    "end": 153,
                    "text": "(Taubenberger and Kash, 2010;",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 154,
                    "end": 176,
                    "text": "Villordo et al., 2015;",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 177,
                    "end": 198,
                    "text": "Longdon et al., 2014)",
                    "ref_id": "BIBREF12"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "In this study, we present a novel approach, using deep neural networks, to predict viral hosts by analyzing either the whole or only fractions of a given viral genome. We selected three different virus species as individual datasets for training and validation of our deep neural networks. These three datasets consist of genomic sequences from influenza A, rabies lyssavirus and rotavirus A, composed of 49, 19, and 6 different known host species, respectively. These known viral hosts are often phylogenetically close related. Previous prediction approaches have combined single species or even genera to higher taxonomical groups to reduce the classification complexity to the price of prediction precision (Zhang et al., 2017; Galiez et al., 2017) . In contrast, our approach is capable of predicting at the host species level, providing much higher accuracy and usefulness of our predictions.",
            "cite_spans": [
                {
                    "start": 710,
                    "end": 730,
                    "text": "(Zhang et al., 2017;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 731,
                    "end": 751,
                    "text": "Galiez et al., 2017)",
                    "ref_id": "BIBREF7"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Our training data consists of at least 100 genomic sequences per virus-host combination. The amount of sequences per combination is unbalanced, which means that some classes are much more common than others. We provide an approach to handle this problem by generating a new balanced training set at each training circle.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Typically the training of recurrent neural networks on very long sequences is very time consuming and inefficient. Truncated backpropagation trough time (TBPTT) (Puskorius and Feldkamp, 1994) tries to solve this problem by splitting the sequences into shorter fragments. We provide a method to regain prediction accuracy lost through this splitting process, leading to fast, efficient learning of long sequences on recurrent neural networks.",
            "cite_spans": [
                {
                    "start": 161,
                    "end": 191,
                    "text": "(Puskorius and Feldkamp, 1994)",
                    "ref_id": "BIBREF15"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "In conclusion, our deep neural network approach is capable of predicting far more complex classification problems than previous approaches (Eng et al., 2014; Kapoor et al., 2010; Zhang et al., 2017; Galiez et al., 2017; Li and Sun, 2018) . Meaning, it is more accurate for the same amount of possible hosts and can predict for more hosts with similar accuracy. Furthermore, our approach does not require any host sequences, which can be helpful due to the limited amount of reference genomes of various species, even ones that are typically known for zoonosis such as ticks and bats (Dilcher et al., 2012; ?; Teeling et al., 2018; Van Zee et al., 2007) .",
            "cite_spans": [
                {
                    "start": 139,
                    "end": 157,
                    "text": "(Eng et al., 2014;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 158,
                    "end": 178,
                    "text": "Kapoor et al., 2010;",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 179,
                    "end": 198,
                    "text": "Zhang et al., 2017;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 199,
                    "end": 219,
                    "text": "Galiez et al., 2017;",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 220,
                    "end": 237,
                    "text": "Li and Sun, 2018)",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 583,
                    "end": 605,
                    "text": "(Dilcher et al., 2012;",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 606,
                    "end": 608,
                    "text": "?;",
                    "ref_id": null
                },
                {
                    "start": 609,
                    "end": 630,
                    "text": "Teeling et al., 2018;",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 631,
                    "end": 652,
                    "text": "Van Zee et al., 2007)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We designed our general workflow to achieve multiple goals: (I) select, preprocess and condense viral sequences with as little information loss as possible (II) correctly handle highly unbalanced datasets to avoid bias during the training phase of the deep neural networks (III) present the output in a clear, user-friendly way while providing as much information as possible.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "General workflow"
        },
        {
            "text": "Our workflow for creating the deep neural networks, used in VIDHOP, to predict viral hosts consisted of five major steps (see Figure 1 ). First, we collected all nucleotide sequences of influenza A, rabies lyssavirus, and rotavirus A with a host label from the European Nucleotide Archive (ENA) database (Leinonen et al., 2010) . We curated the host labels using the taxonomic information provided by the National Center for Biotechnology Information (NCBI), leading to standardized scientific names for the virus taxa and host taxa. Standardization of taxa names enables swift and easy filtering for viruses or hosts on all taxonomic levels. Next, we divide the sequences from the selected hosts and viruses into three sets: the training set, the validation set, and the test set. We provide a solution to use all sequences of an unbalanced dataset without biasing the training in terms of sequences per class while limiting the memory needed to perform this task. Then, the length of the input sequences is equalized to the 0.95 quantile length of the sequences and subsequently further truncated in shorter fragments and parsed into numerical data to facilitate a swift training phase of the deep neural network. After the input preparation, the deep neural network predicts the hosts for the subsequences of the originally provided viral sequences. In the final step, the predictions of the subsequences are analyzed and combined to a general prediction for their respective original sequences.",
            "cite_spans": [
                {
                    "start": 304,
                    "end": 327,
                    "text": "(Leinonen et al., 2010)",
                    "ref_id": "BIBREF10"
                }
            ],
            "ref_spans": [
                {
                    "start": 126,
                    "end": 134,
                    "text": "Figure 1",
                    "ref_id": null
                }
            ],
            "section": "General workflow"
        },
        {
            "text": "Accession numbers of influenza A, rabies lyssavirus, and rotavirus A were collected from the ViPR database (Northrop Grumman Health IT and Technologies, 2017) and Influenza Research Database (for Biotechnology Information, 2017) and all nucleotide sequence information were then downloaded from ENA (status 2018-07-12). From the collected data, we created one dataset per virus species, with all known hosts that had at least 100 sequences. All available sequences were used for each of these hosts (see Figure 2 ).",
            "cite_spans": [
                {
                    "start": 107,
                    "end": 158,
                    "text": "(Northrop Grumman Health IT and Technologies, 2017)",
                    "ref_id": "BIBREF14"
                }
            ],
            "ref_spans": [
                {
                    "start": 504,
                    "end": 512,
                    "text": "Figure 2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Collecting sequences and compiling datasets"
        },
        {
            "text": "To train the deep neural network, we divided each dataset into three smaller subsets, a training set, a validation set, and a test set. Classically, in neuronal network approaches, the data is divided into a ratio of 60 % training set, 20 % validation set, and 20 % test set with a balanced number of data points per class.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Collecting sequences and compiling datasets"
        },
        {
            "text": "Since in our example, nucleotide sequences are the data points, and the different hosts are the classes, this would lead to an unbiased training per host. But for heavily unbalanced datasets, such as typical viral datasets, the majority of sequences would not be used. This is because the host with the smallest number of sequences would determine the maximum usable amount of sequences per host (see Figure 3) .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 401,
                    "end": 410,
                    "text": "Figure 3)",
                    "ref_id": null
                }
            ],
            "section": "Collecting sequences and compiling datasets"
        },
        {
            "text": "A more appropriate approach to deal with large unbalanced datasets is to define a fixed validation set and a fixed test set and create variable training sets from the remaining unassigned . CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page 3 -#3 Anas discors (2243) Anas carolinensis (659) Anas crecca (727) Anas acuta (778) Anas platyrhynchos (24996) Anas clypeata (1085) Branta canadensis (126) Sibirionetta formosa (155) Cairina moschata (990) Struthio camelus (231) Gallus gallus (27229) Meleagris gallopavo (2123) On the left, the potential host species are listed together with the total number of available sequences for these three viruses. The matrix lists the corresponding number of sequences for each virus-host combination used in this study. Dendrograms indicate the phylogenetic relationships of the investigated viruses and host species. As it can be seen, there is a clear imbalance in available viral sequences per virus-host pair.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Collecting sequences and compiling datasets"
        },
        {
            "text": "sequences. In the following, we call this the repeated random undersampling. For each training circle (epoch), a new training set is created by randomly selecting the same number of unassigned sequences per host. The number of selected sequences per host corresponds to the number of unassigned sequences of the smallest class. Repeated random undersampling avoids bias in the training set in terms of sequences per host while using all available sequences. Especially hosts with large quantities of sequences benefit from the generation of many different training sets with random sequence composition.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Collecting sequences and compiling datasets"
        },
        {
            "text": ". CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page 4 -#4",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Collecting sequences and compiling datasets"
        },
        {
            "text": "Florian Mock et al. Figure 3 : Comparison between the classic approach (A) of creating a balanced dataset and the repeated random undersampling (B).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 20,
                    "end": 28,
                    "text": "Figure 3",
                    "ref_id": null
                }
            ],
            "section": "4"
        },
        {
            "text": "In the classic approach, the class with the smallest number of data points defines the number of usable data points for all classes. The repeated random undersampling creates every epoch a new random composition of training data points from all data points which are not included in any of both fixed validation set and test set. For every epoch, the training set is balanced according to the data points per class. The repeated random undersampling can use all available data in an unbalanced dataset, without biasing the training set in terms of data points per class, while limiting the amount of computer memory needed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4"
        },
        {
            "text": "The training data needs to fulfill several properties to be utilizable for neural networks. The input (here, nucleotide sequences) has to be of equal length and also has to be numerical. To achieve this, the length of the sequences was limited to the 0.95 quantile of all sequence lengths by truncating the first positions or in the case of shorter sequences by extension. For sequence extension different strategies were tested and evaluated (see Figure 4 , Supplement Table S2 ):",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 448,
                    "end": 456,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 470,
                    "end": 478,
                    "text": "Table S2",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Input preparation"
        },
        {
            "text": "\u2022 Normal repeat: repeats the original sequence until the 0.95 quantile of all sequence lengths is reached, all redundant bases are truncated. \u2022 Normal repeat with gaps: between one and ten gap symbols are added randomly between each sequence repetition. \u2022 Random repeat: appends the original sequence with slices of the original with the same length as the original. For this purpose, the sequence is treated as a circular list. In a circular list, the end of the sequence is followed by the beginning of the sequence. \u2022 Random repeat with gaps: like random repeat, but repetitions are separated randomly by one to ten gap symbols. \u2022 Append gaps: adds gap symbols at the end of the sequence until the necessary length is reached. \u2022 Online: uses the idea of Online Deep Learning (Sahoo et al., 2017) , i.e., slight modifications to the original training data are introduced and learned by the neuronal network, next to the original training dataset. In our case, randomly selected subsequences of the original sequences are provided as training input. Therefore more diverse data is provided to the neural network. \u2022 Smallest: all sequences are cut to the length of the shortest sequence of the dataset.",
            "cite_spans": [
                {
                    "start": 778,
                    "end": 798,
                    "text": "(Sahoo et al., 2017)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [],
            "section": "Input preparation"
        },
        {
            "text": "After applying one of the mentioned input preparation approaches, each sequence is divided into multiple non-overlapping subsequences (see Figure 4 ). The length of these subsequences ranged between 100 and 400 nucleotides, depending on which length results in the least redundant bases. Using subsequences of a distinct shorter length than the original sequences is a common approach in machine learning to avoid inefficient learning while training long short-term memory networks on very long sequences, see Truncated Backpropagation Through Time approach (Sutskever, 2013) .",
            "cite_spans": [
                {
                    "start": 558,
                    "end": 575,
                    "text": "(Sutskever, 2013)",
                    "ref_id": "BIBREF19"
                }
            ],
            "ref_spans": [
                {
                    "start": 139,
                    "end": 147,
                    "text": "Figure 4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Input preparation"
        },
        {
            "text": "Finally, all subsequences are encoded numerically, using one hot encoding to convert the characters A, C, G, T, -, N into a binary representation (e.g., A = [1, 0, 0, 0, 0], T = [0, 0, 0, 1, 0], \u2212 = [0, 0, 0, 0, 0]). Other characters that may occur in the sequence data were treated as the character N.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Input preparation"
        },
        {
            "text": "Its underlying architecture dramatically determines the performance of the neural network. This architecture needs to be complex enough to use the available information fully but, at the same time, small enough to avoid overfitting effects.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep neural network architecture"
        },
        {
            "text": "All our models (i.e., the combination of the network architecture and various parameter such as the optimizer or validation metrics) were built with the Python (version 3.6.1) package Keras (Chollet et al., 2015) (version 2.2.4) using the Tensorflow (Abadi et al., 2015) (version 1.7) back-end.",
            "cite_spans": [
                {
                    "start": 190,
                    "end": 212,
                    "text": "(Chollet et al., 2015)",
                    "ref_id": null
                },
                {
                    "start": 250,
                    "end": 270,
                    "text": "(Abadi et al., 2015)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Deep neural network architecture"
        },
        {
            "text": "In this study, two different models were built and evaluated to predict viral hosts only given the nucleotide sequence data of the virus (see Figure 5 ). The architecture of our first model consists of a three bidirectional LSTM layers (Hochreiter and Schmidhuber, 1997) , in the following referred to as LSTM architecture (see Figure 5 A). This bidirectional LSTM tries to find longterm context in the input sequence data, presented to the model in forward and reverse direction, which helps to identify interesting patterns for data classification. The LSTM layers are followed by two dense layers were the first collects and combines all calculations of the LSTMs, and the second generates the output layer. Each layer consists of 150 nodes with an exception to the output layer, which has a variable number of nodes. Each node of the output layer represents a possible host species. Since each tested virus dataset contains different numbers of known virus-host pairs, the number of output nodes varies between the different virus datasets. This architecture is similar to those used in text analysis but specifically adjusted to handle long sequences, which are typically problematic for deep learning approaches.",
            "cite_spans": [
                {
                    "start": 236,
                    "end": 270,
                    "text": "(Hochreiter and Schmidhuber, 1997)",
                    "ref_id": "BIBREF8"
                }
            ],
            "ref_spans": [
                {
                    "start": 142,
                    "end": 150,
                    "text": "Figure 5",
                    "ref_id": null
                },
                {
                    "start": 328,
                    "end": 336,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Deep neural network architecture"
        },
        {
            "text": "The second evaluated architecture uses two layers of convolutional neural networks (CNN) nodes, followed by two bidirectional LSTM layers and two dense layers. In the following we will refer to this as the CNN+LSTM architecture (see Figure 5 B ). Similar to the LSTM architecture, each layer consists of 150 nodes with an exception to the output layer. The idea behind this architecture is that the CNN identifies important sequence parts (first layer), combines the short sequence . CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page 5 -#5 Repeat input sequences. Cut off at specified length.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 233,
                    "end": 243,
                    "text": "Figure 5 B",
                    "ref_id": null
                }
            ],
            "section": "Deep neural network architecture"
        },
        {
            "text": "Split each long sequence in multiple subsequences.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep neural network architecture"
        },
        {
            "text": "Parse the sequences. Get samples from subsets. Node type: bidir-LSTM B Figure 5 : Comparison of the two evaluated architectures. The first architecture (A) is similar to neural networks for text analysis. The bidirectional LSTM analyzes the sequence forwards and backward for meaningful patterns, having an awareness of the context as it can remember previously seen data. This architecture is a classic approach for analyzing sequences with temporal information, like literature text, stocks, weather. The second architecture (B) uses CNN nodes, which are common in image recognition, to identify meaningful patterns and combines them into complex features that can then be used by the bidirectional LSTM layers. This architecture is typically used in either more unordered data, such as images, or data with more noise, such as the base-caller output of nanopore sequencing devices (Teng et al., 2018) .",
            "cite_spans": [
                {
                    "start": 884,
                    "end": 903,
                    "text": "(Teng et al., 2018)",
                    "ref_id": "BIBREF23"
                }
            ],
            "ref_spans": [
                {
                    "start": 71,
                    "end": 79,
                    "text": "Figure 5",
                    "ref_id": null
                }
            ],
            "section": "Deep neural network architecture"
        },
        {
            "text": "features to more intricate patterns (second layer), which can then be put into context by the LSTMs, which can remember previously seen patterns.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Deep neural network architecture"
        },
        {
            "text": "The training was done using the repeated random undersampling as described above (see Figure 3 B ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 86,
                    "end": 96,
                    "text": "Figure 3 B",
                    "ref_id": null
                }
            ],
            "section": "Deep neural network training"
        },
        {
            "text": "When given a viral nucleotide sequence, the neural network returns the activation score of the corresponding output nodes of each host. The activation scores of all output nodes add up to 1.0 and can, therefore, be treated as probabilities. Thus, the activation score of each output node represents the likelihood of the corresponding species to serve as a host of the given virus sequence. Due to the splitting of the long sequence into multiple subsequences (see Figure 4) , the neural network predicts potential hosts for every subsequence. The predictions of the subsequences are then combined to the final prediction of the original sequence. Several approaches to combine the subsequence predictions into a final sequence prediction were evaluated (for a detailed example see supplement S1):",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 465,
                    "end": 474,
                    "text": "Figure 4)",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Final host prediction from subsequence predictions"
        },
        {
            "text": "\u2022 Standard: shows the original accuracy for each subsequence.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Final host prediction from subsequence predictions"
        },
        {
            "text": "\u2022 Vote: uses a majority vote on all subsequences to determine the prediction. \u2022 Mean: calculates the mean activation score per class on all subsequences and predicts the class with the highest mean activation. \u2022 Standard deviation: similar to Mean but weights each subsequence with its standard deviation. Subsequences with more distinct predictions get a higher weight.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Final host prediction from subsequence predictions"
        },
        {
            "text": "After combining the subsequence predictions, the single most likely host can be provided as output. However, this limits the prediction power of the neural network. For example, a virus that can survive in two different host species will likely have a high activation score for both hosts. Our tool VIDHOP reports all possible hosts that reach a certain user-defined likelihood, or it can report the n most likely hosts, where n is also a useradjustable parameter.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Final host prediction from subsequence predictions"
        },
        {
            "text": ". CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page 6 -#6",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Final host prediction from subsequence predictions"
        },
        {
            "text": "Florian Mock et al.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6"
        },
        {
            "text": "To evaluate our deep learning approach, we applied it to three different datasets, each containing a great number of either influenza A, rabies lyssavirus or rotavirus A genome sequences, and the respectively known host species. We tested two different architectures and seven different input sequence preparation methods. For all fourteen combinations, a distinct model was trained for a maximum of 1500 epochs. The training was stopped early if the validation accuracy did not increase in the last 300 epochs. For each combination, the prediction accuracy was tested using none or any of the described subsequence prediction approaches.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Discussion"
        },
        {
            "text": "The rotavirus A dataset consists of over 40,000 viral sequences, which are associated with one of six phylogenetically distinct host species. Six different hosts result in an expected random accuracy of \u223c16.67 %. Both tested architectures achieve very high prediction accuracies, even for 239 nucleotide long subsequence (see Table 1 ). The prediction accuracy is influenced not only by the architecture but especially by the input preparation strategy. Overall, the CNN+LSTM architecture achieves higher accuracy than the LSTM architecture with 85.28 %, and 82.88 %, respectively. The highest accuracy was observed with the combination of the CNN+LSTM architecture and the online input preparation. Note that the LSTM architecture has difficulties in learning with some input preparation methods (see LSTM and online). On the one hand, this is probably due to the relatively long input sequence since the LSTM must propagate the error backwards through the entire input sequence and update the weights with the accumulated gradients. The accumulation of gradients over hundreds of nucleotides in the input sequence may cause the values to shrink to zero or result in inflating values (Werbos et al., 1990; Tallec and Ollivier, 2017) . On the other hand, the variability of the online input could enhance the difficulty of finding working features. With prediction accuracies over 82%, on the subsequence level, both the LSTM and the CNN+LSTM architectures indicate that they can identify meaningful classification features. The main differences between the two architectures in the prediction accuracies derive from the applied input preparation strategy. In total, the host prediction quality of rotavirus A sequences achieves an area under the curve (AUC) of 0.98 (see Supplement Figure S3 ). A high AUC is not unsuspected since it is known that rotavirus A shows a distinct adaptation to their respective host (Martella et al., 2010) .",
            "cite_spans": [
                {
                    "start": 1185,
                    "end": 1206,
                    "text": "(Werbos et al., 1990;",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 1207,
                    "end": 1233,
                    "text": "Tallec and Ollivier, 2017)",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1914,
                    "end": 1937,
                    "text": "(Martella et al., 2010)",
                    "ref_id": "BIBREF13"
                }
            ],
            "ref_spans": [
                {
                    "start": 326,
                    "end": 333,
                    "text": "Table 1",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1783,
                    "end": 1792,
                    "text": "Figure S3",
                    "ref_id": "FIGREF6"
                }
            ],
            "section": "Rotavirus A dataset"
        },
        {
            "text": "The rabies lyssavirus dataset consists of more than 12,000 viral sequences, which are associated with 17 different host species, including closely and more distantly related species. This results in an expected random accuracy of \u223c5.88 %. Despite using only a subsequence length of 100 bases, the accuracy of each subsequence prediction is very high (see Table 1 ). The LSTM and the CNN+LSTM architecture reach very similar accuracies with 74.26 %, and 74.39 %, respectively. The differences in prediction accuracy between the architectures per input preparation method are small. An exception from this is the online input preparation method. Similar to the rotavirus A dataset, the LSTM is not able to train well when using the online input preparation method. The highest accuracy per subsequence is reached with the combination of the random repeat input preparation and the CNN+LSTM architecture. Compared to the rotavirus A dataset, the higher amount of host species and closer relation between them makes the rabies lyssavirus dataset harder to predict. In total, the host prediction quality of rabies lyssavirus sequences achieves an AUC of 0.98 (see Supplement Figure S4 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 355,
                    "end": 362,
                    "text": "Table 1",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 1170,
                    "end": 1179,
                    "text": "Figure S4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "Rabies lyssavirus dataset"
        },
        {
            "text": "The influenza A dataset is with more than 213,000 viral sequences and 36 associated possible host species (32 of them are closely related avian species), the most complex of the three evaluated datasets. With 36 different hosts, the expected random accuracy is \u223c2.78 %, which we greatly exceed with an accuracy of over 50 %. The predictions based on 400 nucleotide long influenza A subsequences reached comparable accuracies for nearly all input preparation methods, one notable exception was append gaps, (see Table 1 ). Unlike for the rotavirus A and rabies lyssavirus dataset, the LSTM architecture outperforms the CNN+LSTM architecture with 50.14 %, and 49.40 % host prediction accuracy. Nonetheless, the differences in prediction accuracy between the architectures per input preparation method are again small. The overall best-performing variant is a combination of the LSTM architecture with the normal repeat gaps input preparation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 511,
                    "end": 518,
                    "text": "Table 1",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "Influenza A dataset"
        },
        {
            "text": "The deep neural network achieved an AUC of 0.94 (see supplement Figure S5 ). Despite the close evolutionary distance between the given host species, the trained neuronal network was able to identify potential hosts accurately. We assume that some of the influenza A viruses which are part of the investigated dataset are capable of infecting not only one but several host species, i.e., a single viral sequence can occur in more than one host. However, since we only consider a single host species for each tested viral sequence within the test set, the measured accuracy is most likely an underestimation.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 64,
                    "end": 73,
                    "text": "Figure S5",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "Influenza A dataset"
        },
        {
            "text": "Overall, the host prediction quality for short subsequences for all three datasets is very high, indicating that an accurate prediction of a viral host is possible even if the given viral sequence is only a fraction of the corresponding genome's size. Both architectures are suitable for host prediction, but the more complex the prediction task and the data set, the more favorable the LSTM appears. Nevertheless, for fast prototyping, it makes sense to use CNN+LSTM as it trains around four times faster and reaches comparable results. Furthermore, the CNN+LSTM architecture showed no difficulty in learning long input sequences (see Supplement Figure S1 ). In contrast, the LSTM architecture frequently remained in a state of random accuracy for a long time during training (see Supplement Figure S2 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 647,
                    "end": 656,
                    "text": "Figure S1",
                    "ref_id": null
                },
                {
                    "start": 793,
                    "end": 802,
                    "text": "Figure S2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "Best practice and useful observations"
        },
        {
            "text": "We observed random repeat gaps and normal repeat gaps to be the most suited input preparations for the LSTM architecture, as they achieved the highest accuracies. The final selection of the best working input preparation seems to depend on the virus species.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Best practice and useful observations"
        },
        {
            "text": "The random repeat gaps approach provides the neural network with an almost random selection of the original sequence. All selections are separated by gaps. The first subsequences are . CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint With the normal repeat gaps approach, the neural network can identify not just the start but also the end of the original sequence because the ends are marked by gaps. This may provide a useful context detection for the LSTM layer.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Best practice and useful observations"
        },
        {
            "text": "A completely random selection, as in the online approach, seems to be too diverse. Non-random approaches such as normal repeat seem to lead to faster overfitting of the training set, thus limiting the ability of the deep neural network to identify general usable features.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Best practice and useful observations"
        },
        {
            "text": "Smallest and append gaps proved to be unsuitable methods for input preparation. Here append gaps leads to a prediction of subsequences without usable information because they consist only of gaps, whereas smallest limits the available information so much that a prediction becomes inaccurate.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Best practice and useful observations"
        },
        {
            "text": "Among the tested combination approaches of the subsequencepredictions, std-div was observed to perform best (see Table 2 ).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 113,
                    "end": 120,
                    "text": "Table 2",
                    "ref_id": "TABREF4"
                }
            ],
            "section": "Combining subsequence host predictions results in higher accuracy"
        },
        {
            "text": "With the combination of all subsequence predictions, the accuracy rises between 2.2 %-4.6 %, with a mean increase of 3.2 %. This result shows that a combination of the host prediction results of all subsequences of a given viral sequence can increase the overall prediction accuracy. Presumably, the prediction combination approaches can compensate for the possible information loss caused by the sequence splitting process during the input preparation.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Combining subsequence host predictions results in higher accuracy"
        },
        {
            "text": "Besides evaluating our deep learning approach on the three virus datasets, we compared our results with a similar study. Our approach predicts hosts on the species level, whereas most other studies are limited to predicting the host genera (Zhang et al., 2017; Galiez et al., 2017) or even higher taxonomic groups (Eng et al., 2014; Kapoor et al., 2010) . In a relatively comparable study, Le et al. (Li and Sun, 2018 ) also tried to predict potential hosts on species level for influenza A and rabies lyssaviruses. In their study, they mainly tested three different approaches, which were mostly combinations of already published methods Zhang et al., 2017; Kapoor et al., 2010) , including a support vector machine approach and two sequence similarity approaches, one of which alignment-based, the other without alignment. These three methods represent the state of the art.",
            "cite_spans": [
                {
                    "start": 240,
                    "end": 260,
                    "text": "(Zhang et al., 2017;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 261,
                    "end": 281,
                    "text": "Galiez et al., 2017)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 314,
                    "end": 332,
                    "text": "(Eng et al., 2014;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 333,
                    "end": 353,
                    "text": "Kapoor et al., 2010)",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 400,
                    "end": 417,
                    "text": "(Li and Sun, 2018",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 639,
                    "end": 658,
                    "text": "Zhang et al., 2017;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 659,
                    "end": 679,
                    "text": "Kapoor et al., 2010)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "VIDHOP outperforms other approaches"
        },
        {
            "text": "The rabies lyssavirus dataset from Le et al. consisted of 148 viruses and 19 associated bat host species. Our rabies lyssavirus dataset consists of 12,025 viruses and has 17 associated host species, but none of them is a bat species. Since both data sets do not have a common species, the difficulty of both prediction tasks is difficult to estimate when compared to each other, and therefore comparability is limited. Le et al. reached an accuracy below 79 % on the bat species dataset with the similarity-based approaches, one of which relies on alignment data and vice versa. The SVM reached an accuracy below 76 %. On our dataset, VIDHOP reached a similar accuracy of around 77 %. However, in contrast to our analysis, Le et al. used an unbalanced dataset, which often leads to an overestimation of the prediction accuracy. Furthermore, the presented accuracy from Le et al. is based on n-fold cross-validation. Nfold cross-validation lowers the comparability with other studies since the quasi-standard for accuracy determination is 10-fold cross-validation. When applying 10-fold cross-validation, their host prediction accuracy for their rabies lyssavirus dataset drops under 65 %.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIDHOP outperforms other approaches"
        },
        {
            "text": "The influenza A dataset from Le et al. consisted of 1,200 viral sequences and six associated host species. For this dataset, they reached a host prediction accuracy of below 61 % with the alignment-free method (below 40 % when applying 10-fold crossvalidation). The alignment-based method reached bellow 52 %, and the SVM bellow 57 %. Our influenza A dataset consists of 211,679 viral sequences and has 36 associated host species, including the six species from the Le et al. dataset. Our deep learning approach reached a host prediction accuracy of 54.31 %, which is very good, given that we had to predict six times the number of potential host species with closer phylogenetic relationships among them. To reach better comparability by considering the number of classes, we calculated the average accuracy (see Equation 1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIDHOP outperforms other approaches"
        },
        {
            "text": "With an average accuracy of 97.46 % VIDHOP surpassed the previous methods which reached an average accuracy between 84 % and 87 % (see Table 3 ). To our best knowledge, no comparable study exists for the rotavirus A dataset.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 135,
                    "end": 142,
                    "text": "Table 3",
                    "ref_id": null
                }
            ],
            "section": "VIDHOP outperforms other approaches"
        },
        {
            "text": "VIDHOP reached on all three datasets a very high average accuracy between 96.11 % for rotavirus A, 97.30 % for rabies lyssavirus and 97.46 % for influenza A (see Table 4 ). These results indicate the versatility of the presented deep learning approach for the task of host prediction.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 162,
                    "end": 169,
                    "text": "Table 4",
                    "ref_id": null
                }
            ],
            "section": "VIDHOP outperforms other approaches"
        },
        {
            "text": ". CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page 8 -#8",
            "cite_spans": [],
            "ref_spans": [],
            "section": "VIDHOP outperforms other approaches"
        },
        {
            "text": "Florian Mock et al. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "8"
        },
        {
            "text": "In this study, we presented the tool VIDHOP and investigated the usability of deep learning for the prediction of hosts for distinct viruses, based on the viral nucleotide sequences alone. We established a simple but very capable prediction pipeline, including possible data preparation steps, data training strategies, and a suitable deep neural network architecture. Besides, we provide three different neural network models, which can predict potential hosts for either influenza A, rotavirus A, or rabies lyssavirus, respectively. These deep neural networks are used in VIDHOP and use genomic fragments shorter than 400 nucleotides to predict potential virus hosts directly on a species level. In contrast to similar approaches, this is a more complex task than performing host prediction only on the genera level (Zhang et al., 2017; Galiez et al., 2017) or even higher taxonomic groups (Eng et al., 2014; Kapoor et al., 2010) . Moreover, our approach can predict more hosts with comparable accuracy than previous approaches. The consistently high average accuracy of VIDHOP, on all three datasets, indicates the versatility of the deep learning approach we used. Additionally, we addressed multiple problems that arise when using DNA or RNA sequences as input for deep learning, such as unbalanced datasets for training and the problem of inefficient learning of recurrent neural networks (RNN) on long sequences. We evaluated different solutions to solve these problems and observed that splitting of the original virus genome sequence in combination with merging the prediction results of the generated subsequences leads to fast and efficient learning on long sequences. Furthermore, the use of unbalanced datasets is possible if a new balanced training set is generated by repeated random undersampling (a random selection of available sequences) for every single epoch during the training phase.",
            "cite_spans": [
                {
                    "start": 818,
                    "end": 838,
                    "text": "(Zhang et al., 2017;",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 839,
                    "end": 859,
                    "text": "Galiez et al., 2017)",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 892,
                    "end": 910,
                    "text": "(Eng et al., 2014;",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 911,
                    "end": 931,
                    "text": "Kapoor et al., 2010)",
                    "ref_id": "BIBREF9"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "With the use of deep neural networks for host predicting of viruses, it is possible to rapidly identify the host, without the use of arbitrarily selected learning features, for a large number of host species. This allows us to identify the original host of zoonotic events and makes it possible to swiftly limit the intensity of a viral outbreak by separating the original host from humans or livestock.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "In future approaches, it could be interesting to investigate the use of newly developed deep neural network layers, such as transformer self-attention layers (Vaswani et al., 2017) . This layer type has been shown to perform well with character sequences (Al-Rfou et al., 2018) , such as DNA or RNA sequences, potentially allowing for a further increase in the prediction quality. The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page S1 -#10 host prediction S1",
            "cite_spans": [
                {
                    "start": 158,
                    "end": 180,
                    "text": "(Vaswani et al., 2017)",
                    "ref_id": "BIBREF24"
                },
                {
                    "start": 255,
                    "end": 277,
                    "text": "(Al-Rfou et al., 2018)",
                    "ref_id": "BIBREF2"
                }
            ],
            "ref_spans": [],
            "section": "Conclusion"
        },
        {
            "text": "Epochs 0 . CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page S2 -#11",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Supplementary Information"
        },
        {
            "text": "Florian Mock et al. Table S1 . Comparison of host prediction results in regards to the different approaches that can be used to merge prediction scores of subsequences.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 20,
                    "end": 28,
                    "text": "Table S1",
                    "ref_id": "TABREF3"
                }
            ],
            "section": "S2"
        },
        {
            "text": "In this example, a viral nucleotide sequence was split into five subsequences, and each of them was used to predict the corresponding host. Depending on the subsequence activation score merging approach, the final host prediction can vary. Human Human Avian Avian Human Human Avian Avian Table S2 . Comparison of the functional principle of the input expansion. Note that for real data, the raw data sequence would be hundreds of bases long. Furthermore, in this example, ACT is the shortest sequence of our dataset, but still longer than the input length expected by the neural network. Figure S4 : ROC curve for the rabies dataset, calculated on the test set.The AUC of the micro-average ROC curve is 0.98 and for the macro-average 0.97. The trade-off between False Positive Rate and True Positive Rate, when using mean vote, is shown with a green dot.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 288,
                    "end": 296,
                    "text": "Table S2",
                    "ref_id": "TABREF4"
                },
                {
                    "start": 588,
                    "end": 597,
                    "text": "Figure S4",
                    "ref_id": "FIGREF3"
                }
            ],
            "section": "S2"
        },
        {
            "text": ". CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page S3 -#12 . CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. (703) Vulpes lagopus (198) Nyctereutes procyonoides (121) Canis lupus (5566) Cerdocyon thous (102) Mephitis mephitis (692) Felis catus (279) Procyon lotor (578) Desmodus rotundus (184) Artibeus lituratus (110) Tadarida brasiliensis (265) Lasiurus borealis (116) Eptesicus fuscus (480) Vicugna pacos (118) Sus scrofa (42440) Bos taurus (3516) Capra hircus (120) Homo sapiens (136031) Equus caballus (2469) Uria aalge (198) Arenaria interpres (3374) Calidris ruficollis (105) Calidris canutus (250) Calidris alba (127) Larus argentatus (306) Larus glaucescens (152) Leucophaeus atricilla (290) Chroicocephalus ridibundus (1476) Anser fabalis (201) Anser albifrons (291) Anser indicus (105) Cygnus cygnus (110) Cygnus olor (242) Cygnus columbianus (221) Mareca penelope (248) Mareca strepera (132) Mareca americana (154) Tadorna ferruginea (204) Anas rubripes (346) Anas discors (2243) Anas carolinensis (659) Anas crecca (727) Anas acuta (778) Anas platyrhynchos ( ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Subsequences"
        }
    ],
    "bib_entries": {
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Alignment-free d * 2 oligonucleotide frequency dissimilarity measure improves prediction of hosts from metagenomicallyderived viral sequences",
            "authors": [
                {
                    "first": "N",
                    "middle": [
                        "A"
                    ],
                    "last": "Ahlgren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [
                        "Y"
                    ],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Fuhrman",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Nucleic Acids Res",
            "volume": "45",
            "issn": "",
            "pages": "39--53",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Character-level language modeling with deeper self-attention",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Al-Rfou",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Choe",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Constant",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Guo",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1808.04444"
                ]
            }
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "Genetic characterization of Tribe\u010d virus and Kemerovo virus, two tick-transmitted human-pathogenic Orbiviruses",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Dilcher",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Hasib",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lechner",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Wieseke",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Middendorf",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Marz",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Koch",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Spiegel",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Dobler",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "T"
                    ],
                    "last": "Hufert",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Weidmann",
                    "suffix": ""
                }
            ],
            "year": 2012,
            "venue": "Virology",
            "volume": "423",
            "issn": "1",
            "pages": "68--76",
            "other_ids": {}
        },
        "BIBREF5": {
            "ref_id": "b5",
            "title": "Computational approaches to predict bacteriophage-host relationships",
            "authors": [
                {
                    "first": "R",
                    "middle": [
                        "A"
                    ],
                    "last": "Edwards",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Mcnair",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Faust",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Raes",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [
                        "E"
                    ],
                    "last": "Dutilh",
                    "suffix": ""
                }
            ],
            "year": 2016,
            "venue": "FEMS Microbiol Rev",
            "volume": "40",
            "issn": "",
            "pages": "258--272",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "Predicting host tropism of influenza a virus proteins using random forest",
            "authors": [
                {
                    "first": "C",
                    "middle": [
                        "L P"
                    ],
                    "last": "Eng",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Tong",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [
                        "W"
                    ],
                    "last": "Tan",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "BMC Med Genomics",
            "volume": "7",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "WIsH: who is the host? predicting prokaryotic hosts from metagenomic phage contigs",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Galiez",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Siebert",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Enault",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Vincent",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "S\u00f6ding",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Bioinformatics",
            "volume": "33",
            "issn": "19",
            "pages": "3113--3114",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "Long short-term memory",
            "authors": [
                {
                    "first": "S",
                    "middle": [],
                    "last": "Hochreiter",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Schmidhuber",
                    "suffix": ""
                }
            ],
            "year": 1997,
            "venue": "Neural computation",
            "volume": "9",
            "issn": "8",
            "pages": "1735--1780",
            "other_ids": {}
        },
        "BIBREF9": {
            "ref_id": "b9",
            "title": "Use of nucleotide composition analysis to infer hosts for three novel picorna-like viruses",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Kapoor",
                    "suffix": ""
                },
                {
                    "first": "P",
                    "middle": [],
                    "last": "Simmonds",
                    "suffix": ""
                },
                {
                    "first": "W",
                    "middle": [
                        "I"
                    ],
                    "last": "Lipkin",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Zaidi",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Delwart",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Journal of virology",
            "volume": "84",
            "issn": "",
            "pages": "10322--10328",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "The european nucleotide archive",
            "authors": [
                {
                    "first": "R",
                    "middle": [],
                    "last": "Leinonen",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Akhtar",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Birney",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Bower",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "Cerdeno-T\u00e1rraga",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Cheng",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Cleland",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Faruque",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Goodgame",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Gibson",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Nucleic acids research",
            "volume": "39",
            "issn": "suppl_1",
            "pages": "28--31",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "Comparative studies of alignment, alignment-free and svm based approaches for predicting the hosts of viruses based on viral sequences",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Li",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Sci Rep",
            "volume": "8",
            "issn": "1",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "The evolution and genetics of virus host shifts",
            "authors": [
                {
                    "first": "B",
                    "middle": [],
                    "last": "Longdon",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "A"
                    ],
                    "last": "Brockhurst",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "A"
                    ],
                    "last": "Russell",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "J"
                    ],
                    "last": "Welch",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [
                        "M"
                    ],
                    "last": "Jiggins",
                    "suffix": ""
                }
            ],
            "year": 2014,
            "venue": "PLoS pathogens",
            "volume": "10",
            "issn": "11",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "Zoonotic aspects of rotaviruses",
            "authors": [
                {
                    "first": "V",
                    "middle": [],
                    "last": "Martella",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "B\u00e1nyai",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Matthijnssens",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Buonavoglia",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Ciarlet",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Veterinary microbiology",
            "volume": "140",
            "issn": "3-4",
            "pages": "246--255",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "Virus pathogen resource viprbrc.org/. [Online; Stand 18",
            "authors": [
                {
                    "first": "Northrop",
                    "middle": [
                        "Grumman"
                    ],
                    "last": "Health",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [
                        "T"
                    ],
                    "last": "",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C V I"
                    ],
                    "last": "Technologies",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Truncated backpropagation through time and kalman filter training for neurocontrol",
            "authors": [
                {
                    "first": "G",
                    "middle": [],
                    "last": "Puskorius",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Feldkamp",
                    "suffix": ""
                }
            ],
            "year": 1994,
            "venue": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)",
            "volume": "4",
            "issn": "",
            "pages": "2488--2493",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "Investigating the zoonotic origin of the west african ebola epidemic",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "M"
                    ],
                    "last": "Sa\u00e9z",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Weiss",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Nowak",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Lapeyre",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Zimmermann",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [],
                    "last": "D\u00fcx",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [
                        "S"
                    ],
                    "last": "K\u00fchl",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Kaba",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Regnaut",
                    "suffix": ""
                },
                {
                    "first": "K",
                    "middle": [],
                    "last": "Merkel",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "EMBO molecular medicine",
            "volume": "7",
            "issn": "1",
            "pages": "17--23",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Online deep learning: Learning deep neural networks on the fly",
            "authors": [
                {
                    "first": "D",
                    "middle": [],
                    "last": "Sahoo",
                    "suffix": ""
                },
                {
                    "first": "Q",
                    "middle": [],
                    "last": "Pham",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Lu",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C H"
                    ],
                    "last": "Hoi",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "A systematic analysis of performance measures for classification tasks. Information processing & management",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Sokolova",
                    "suffix": ""
                },
                {
                    "first": "G",
                    "middle": [],
                    "last": "Lapalme",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "",
            "volume": "45",
            "issn": "",
            "pages": "427--437",
            "other_ids": {}
        },
        "BIBREF19": {
            "ref_id": "b19",
            "title": "Training recurrent neural networks",
            "authors": [
                {
                    "first": "I",
                    "middle": [],
                    "last": "Sutskever",
                    "suffix": ""
                }
            ],
            "year": 2013,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF20": {
            "ref_id": "b20",
            "title": "Unbiasing truncated backpropagation through time",
            "authors": [
                {
                    "first": "C",
                    "middle": [],
                    "last": "Tallec",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Ollivier",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {
                "arXiv": [
                    "arXiv:1705.08209"
                ]
            }
        },
        "BIBREF21": {
            "ref_id": "b21",
            "title": "Influenza virus evolution, host adaptation, and pandemic formation",
            "authors": [
                {
                    "first": "J",
                    "middle": [
                        "K"
                    ],
                    "last": "Taubenberger",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "C"
                    ],
                    "last": "Kash",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "Cell Host Microbe",
            "volume": "7",
            "issn": "6",
            "pages": "440--451",
            "other_ids": {}
        },
        "BIBREF22": {
            "ref_id": "b22",
            "title": "Bat biology, genomes, and the bat1k project: To generate chromosome-level genomes for all living bat species",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "C"
                    ],
                    "last": "Teeling",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [
                        "C"
                    ],
                    "last": "Vernes",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "M"
                    ],
                    "last": "D\u00e1valos",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "A"
                    ],
                    "last": "Ray",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "T P"
                    ],
                    "last": "Gilbert",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [],
                    "last": "Myers",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Consortium",
                    "suffix": ""
                }
            ],
            "year": 2018,
            "venue": "Annual review of animal biosciences",
            "volume": "6",
            "issn": "",
            "pages": "23--46",
            "other_ids": {}
        },
        "BIBREF23": {
            "ref_id": "b23",
            "title": "Chiron: translating nanopore raw signal directly into nucleotide sequence using deep learning",
            "authors": [
                {
                    "first": "H",
                    "middle": [],
                    "last": "Teng",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "D"
                    ],
                    "last": "Cao",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "B"
                    ],
                    "last": "Hall",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Duarte",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [
                        "J"
                    ],
                    "last": "Coin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "P"
                    ],
                    "last": "Van Zee",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Geraci",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Guerrero",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Wikel",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Stuart",
                    "suffix": ""
                },
                {
                    "first": "V",
                    "middle": [],
                    "last": "Nene",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Hill",
                    "suffix": ""
                }
            ],
            "year": 2007,
            "venue": "International journal for parasitology",
            "volume": "7",
            "issn": "5",
            "pages": "1297--1305",
            "other_ids": {}
        },
        "BIBREF24": {
            "ref_id": "b24",
            "title": "Attention is all you need",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Vaswani",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Shazeer",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [],
                    "last": "Parmar",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Uszkoreit",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Jones",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "N"
                    ],
                    "last": "Gomez",
                    "suffix": ""
                },
                {
                    "first": "\u0141",
                    "middle": [],
                    "last": "Kaiser",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "Polosukhin",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "Advances in Neural Information Processing Systems",
            "volume": "",
            "issn": "",
            "pages": "5998--6008",
            "other_ids": {}
        },
        "BIBREF25": {
            "ref_id": "b25",
            "title": "Dengue virus rna structure specialization facilitates host adaptation",
            "authors": [
                {
                    "first": "S",
                    "middle": [
                        "M"
                    ],
                    "last": "Villordo",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "V"
                    ],
                    "last": "Filomatori",
                    "suffix": ""
                },
                {
                    "first": "I",
                    "middle": [],
                    "last": "S\u00e1nchez-Vargas",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "D"
                    ],
                    "last": "Blair",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "V"
                    ],
                    "last": "Gamarnik",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "PLoS Pathog",
            "volume": "11",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF26": {
            "ref_id": "b26",
            "title": "Backpropagation through time: what it does and how to do it",
            "authors": [
                {
                    "first": "P",
                    "middle": [
                        "J"
                    ],
                    "last": "Werbos",
                    "suffix": ""
                }
            ],
            "year": 1990,
            "venue": "Proceedings of the IEEE",
            "volume": "78",
            "issn": "10",
            "pages": "1550--1560",
            "other_ids": {}
        },
        "BIBREF27": {
            "ref_id": "b27",
            "title": "Prediction of virus-host infectious association by supervised learning methods",
            "authors": [
                {
                    "first": "M",
                    "middle": [],
                    "last": "Zhang",
                    "suffix": ""
                },
                {
                    "first": "L",
                    "middle": [],
                    "last": "Yang",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Ren",
                    "suffix": ""
                },
                {
                    "first": "N",
                    "middle": [
                        "A"
                    ],
                    "last": "Ahlgren",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "A"
                    ],
                    "last": "Fuhrman",
                    "suffix": ""
                },
                {
                    "first": "F",
                    "middle": [],
                    "last": "Sun",
                    "suffix": ""
                }
            ],
            "year": 2017,
            "venue": "BMC Bioinf",
            "volume": "18",
            "issn": "",
            "pages": "",
            "other_ids": {}
        }
    },
    "ref_entries": {
        "FIGREF1": {
            "text": "At the top, the examined virus species are listed with the total amount of used nucleotide sequences shown in brackets.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Conversion of input sequences into numerical data of equal length using the normal repeat method. Each sequence is extended through self-repetition and is then trimmed to the 0.95 quantile sequence length. Sequences are then split into multiple non-overlapping subsequences of equal length. Each subsequence is then converted via one hot encoding into a list of numerical vectors.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Conceptualization: FM, AV and MM. Data curation: FM. Formal analysis: FM. Data interpretation: FM. Methodology: FM, AV and MM. Validation: FM. Visualization: FM. Writing -Original Draft Preparation: FM. Writing -Review & Editing: EB. Project Administration: MM. Supervision: MM. Funding acquisition: MM.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Training accuracy and validation accuracy over 1000 epochs on the influenza A dataset. With the CNN+LSTM architecture, the deep neural network shows no difficulties to learn. Training accuracy and validation accuracy over 1000 epochs on the influenza A dataset. With the LSTM architecture, the deep neural network remained in a state of random accuracy for ca. 200 epochs.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF6": {
            "text": "ROC curve for the rota dataset, calculated on the test set. The AUC of the micro-average ROC curve is 0.97 and for the macro-average 0.98. The trade-off between False Positive Rate and True Positive Rate, when using mean vote, is shown with a green dot.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "ROC curve for the influenza dataset, calculated on the test set. The AUC of the micro-average ROC curve is 0.94 and for the macro-average 0.94. The trade-off between False Positive Rate and True Positive Rate, when using mean vote, is shown with a green dot.",
            "latex": null,
            "type": "figure"
        },
        "TABREF0": {
            "text": "Figure 1: The general workflow consists of several steps. First, suitable viral sequences have to be collected and standardized. Next, these sequences will be distributed into the training set, validation set, and test set. The sequences are then adjusted in length and are parsed into numerical data, which is then used to train the deep neural network. The neural network predicts the host for multiple subsequences of the original input. The subsequence predictions are then combined to a final prediction. The workflow used in VIDHOP to predict new sequences is framed in black.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "), i.e., having a fixed validation set and test set while the training set was newly compiled during each epoch. All classes had an equal amount of sequences during each epoch. This balancing step results in an After training, the model weights with the lowest validation loss and model weights with the highest validation accuracy were applied for predicting the test set.",
            "latex": null,
            "type": "table"
        },
        "TABREF3": {
            "text": "Host prediction accuracy in percent on the rotavirus A, rabies lyssavirus, and influenza A dataset with different architectures and input preparation strategies. The input preparation strategy with the highest accuracy for each architecture is marked dark grey, the second-best in light grey. Expected accuracy by chance is \u223c16.67 % for rotavirus A, \u223c5.88 % for rabies lyssavirus and \u223c2.78 % for influenza A.",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "Percentage accuracy of the best working input preparation with respect to the combination methods of the subsequence predictions. The combination method with the highest accuracy for each combination is marked in grey.",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "The authors declare no competing interests. . CC-BY-NC 4.0 International license The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. It is made available under a . https://doi.org/10.1101/575571 doi: bioRxiv preprint \"host_pred_arxiv\" -2020/1/31 -page 9 -#9 Anas clypeata (1085) Branta canadensis (126) Sibirionetta formosa (155) Cairina moschata (990) Struthio camelus (231) Gallus gallus (27229) Meleagris gallopavo (2123) ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Competing Interests"
        }
    ]
}